# Test to do

- Train an RNN for the 1st one and apply this RNN on the 2nd one and see the performances



# TO DO

- Train model (LSTM OK)
- Classic Encoder => Decoder RNN architecture
- RNN with attention (Nearby OK)
- Transformers
- Initialization of the weights of the RNNs
- Teacher training
- Make a toy example to test the architectures

## Command on vega

```bash
nohup python3 main.py -c_t model/LSTM/10.model -t --rnn LSTM > file.out 2> err.log &
```



